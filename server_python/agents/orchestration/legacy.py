#!/usr/bin/env python3
"""
Production-grade Orchestration ëª¨ë“ˆ
ë©€í‹°-ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬, ì‹¤í–‰ ë£¨í”„, LLM í˜¸ì¶œ ìµœì í™”
"""
import asyncio
import aiohttp
import os
import json
from abc import ABC, abstractmethod
from typing import Dict, List, Optional, Any, Callable, Union
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from contextlib import asynccontextmanager

# =============================================================================
# LLM Client (Connection Pooling, Retry, Timeout)
# =============================================================================

class LLMClient:
    """
    LLM API í´ë¼ì´ì–¸íŠ¸ - Connection pooling, retry, timeout ì§€ì›
    Singleton íŒ¨í„´ìœ¼ë¡œ ì „ì—­ ì¬ì‚¬ìš©
    """
    _instance: Optional['LLMClient'] = None
    _session: Optional[aiohttp.ClientSession] = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        self._initialized = getattr(self, '_initialized', False)
        if self._initialized:
            return  # Singleton: ì´ë¯¸ ì´ˆê¸°í™”ë¨
        
        self.api_url = os.getenv("LLM_API_URL", "https://api.platform.a15t.com/v1/chat/completions")
        self.api_key = os.getenv("LLM_API_KEY", "")
        self.model = os.getenv("LLM_MODEL", "azure/openai/gpt-4o")
        self.default_temperature = float(os.getenv("LLM_TEMPERATURE", "1.0"))
        self.max_tokens = int(os.getenv("LLM_MAX_TOKENS", "8000"))
        self.timeout = aiohttp.ClientTimeout(total=120, connect=10)  # timeout ì¦ê°€
        self.max_retries = 3
        self.retry_delay = 1.0
        self._initialized = True
        
        # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ìƒíƒœ ì¶œë ¥
        self._print_config()
    
    def _print_config(self):
        """í˜„ì¬ ì„¤ì • ì¶œë ¥"""
        api_key_status = "âœ… ì„¤ì •ë¨" if self.api_key else "âŒ ë¯¸ì„¤ì •"
        print(f"[LLMClient] API URL: {self.api_url}")
        print(f"[LLMClient] API Key: {api_key_status}")
        print(f"[LLMClient] Model: {self.model}")
        print(f"[LLMClient] Temperature: {self.default_temperature}")
        print(f"[LLMClient] Max Tokens: {self.max_tokens}")
    
    def update_config(self, provider: str = None, model: str = None, api_key: str = None, 
                     base_url: str = None, temperature: float = None, max_tokens: int = None):
        """
        í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì „ë‹¬ë°›ì€ LLM ì„¤ì •ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        """
        updated = []
        
        if base_url is not None and base_url.strip():
            # base_urlì— /chat/completionsê°€ ì—†ìœ¼ë©´ ì¶”ê°€
            base_url = base_url.strip()
            if not base_url.endswith('/chat/completions'):
                if base_url.endswith('/'):
                    base_url = base_url + 'chat/completions'
                elif base_url.endswith('/v1'):
                    base_url = base_url + '/chat/completions'
                elif '/v1' in base_url:
                    base_url = base_url.rstrip('/') + '/chat/completions'
                else:
                    base_url = base_url.rstrip('/') + '/v1/chat/completions'
            
            if base_url != self.api_url:
                self.api_url = base_url
                updated.append(f"api_url={base_url}")
        
        if api_key is not None and api_key != self.api_key:
            self.api_key = api_key
            updated.append("api_key=***")
        
        if model is not None and model != self.model:
            # modelì´ ì´ë¯¸ provider/model í˜•ì‹ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ì˜ˆ: "azure/openai/gpt-5-2025-08-07-gs" -> ê·¸ëŒ€ë¡œ
            # ì˜ˆ: "gpt-4o" + provider="openai" -> "openai/gpt-4o" (í•„ìš”ì‹œ)
            # í•˜ì§€ë§Œ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì´ë¯¸ ì¡°í•©ëœ í˜•ì‹ìœ¼ë¡œ ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ìœ¼ë¯€ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if '/' in model:
                # ì´ë¯¸ provider/model í˜•ì‹
                self.model = model
            elif provider:
                # providerì™€ model ë¶„ë¦¬ëœ ê²½ìš° ì¡°í•©
                self.model = f"{provider}/{model}"
            else:
                # modelë§Œ ìˆëŠ” ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                self.model = model
            updated.append(f"model={self.model}")
        
        if temperature is not None and temperature != self.default_temperature:
            self.default_temperature = temperature
            updated.append(f"temperature={temperature}")
        
        if max_tokens is not None and max_tokens != self.max_tokens:
            self.max_tokens = max_tokens
            updated.append(f"max_tokens={max_tokens}")
        
        if updated:
            print(f"[LLMClient] ì„¤ì • ì—…ë°ì´íŠ¸: {', '.join(updated)}")
            self._print_config()
        
        return len(updated) > 0
    
    async def _get_session(self) -> aiohttp.ClientSession:
        """ì„¸ì…˜ ì¬ì‚¬ìš© (Connection pooling)"""
        if self._session is None or self._session.closed:
            connector = aiohttp.TCPConnector(limit=20, limit_per_host=10)
            self._session = aiohttp.ClientSession(
                connector=connector,
                timeout=self.timeout
            )
        return self._session
    
    async def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if self._session and not self._session.closed:
            await self._session.close()
            self._session = None
    
    def _is_fixed_temperature_model(self) -> bool:
        """temperature=1ë§Œ ì§€ì›í•˜ëŠ” ëª¨ë¸ì¸ì§€ í™•ì¸"""
        model_lower = self.model.lower()
        # o1, o3 reasoning ëª¨ë¸ ë° ì¼ë¶€ íŠ¹ìˆ˜ ëª¨ë¸ì€ temperature=1ë§Œ ì§€ì›
        fixed_temp_patterns = [
            'o1', 'o3', 'o1-', 'o3-', '/o1', '/o3',
            'gpt-5', 'gpt5',  # GPT-5 ê³„ì—´
        ]
        return any(pattern in model_lower for pattern in fixed_temp_patterns)

    async def call(
        self,
        messages: List[Dict[str, str]],
        max_tokens: int = 1000,
        temperature: float = None,  # Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        json_mode: bool = False
    ) -> str:
        """
        LLM API í˜¸ì¶œ with retry & timeout
        """
        if not self.api_key:
            print("[LLM] Warning: LLM_API_KEY not set")
            return '{"error": "LLM API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."}'

        # temperatureê°€ Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        actual_temperature = temperature if temperature is not None else self.default_temperature

        # ì¼ë¶€ ëª¨ë¸ì€ temperature=1ë§Œ ì§€ì›
        if self._is_fixed_temperature_model() and actual_temperature != 1.0:
            print(f"[LLM] Fixed-temperature model detected ({self.model}), forcing temperature=1.0")
            actual_temperature = 1.0
        
        session = await self._get_session()
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        payload = {
            "model": self.model,
            "messages": messages,
            "max_completion_tokens": max_tokens,
            "temperature": actual_temperature,
            "stream": False
        }
        
        if json_mode:
            payload["response_format"] = {"type": "json_object"}
        
        last_error = None
        print(f"[LLM] Calling API: {self.api_url}, model={self.model}, messages={len(messages)}")
        for attempt in range(self.max_retries):
            try:
                async with session.post(
                    self.api_url,
                    headers=headers,
                    json=payload
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        print(f"[LLM] Raw API response: {json.dumps(data, ensure_ascii=False)[:500]}...")
                        
                        # Azure OpenAI / OpenAI í˜•ì‹
                        content = ""
                        if "choices" in data and data["choices"]:
                            choice = data["choices"][0]
                            if "message" in choice:
                                content = choice["message"].get("content", "")
                            elif "text" in choice:
                                content = choice["text"]
                        
                        # Anthropic í˜•ì‹ ëŒ€ì‘
                        if not content and "content" in data:
                            if isinstance(data["content"], list):
                                for item in data["content"]:
                                    if item.get("type") == "text":
                                        content = item.get("text", "")
                                        break
                            elif isinstance(data["content"], str):
                                content = data["content"]
                        
                        print(f"[LLM] Parsed content: {len(content)} chars, preview: {content[:100] if content else 'EMPTY'}...")
                        return content
                    elif response.status == 429:  # Rate limit
                        retry_after = float(response.headers.get("Retry-After", self.retry_delay * (attempt + 1)))
                        print(f"[LLM] Rate limited, waiting {retry_after}s...")
                        await asyncio.sleep(retry_after)
                        continue
                    else:
                        error_text = await response.text()
                        last_error = f"API Error ({response.status}): {error_text}"
                        print(f"[LLM] {last_error}")
                        
            except asyncio.TimeoutError:
                last_error = "Timeout"
                print(f"[LLM] Timeout on attempt {attempt + 1}")
            except Exception as e:
                last_error = str(e)
                print(f"[LLM] Error on attempt {attempt + 1}: {e}")
            
            if attempt < self.max_retries - 1:
                await asyncio.sleep(self.retry_delay * (attempt + 1))
        
        return json.dumps({"error": last_error or "Unknown error"})


# ì „ì—­ LLM í´ë¼ì´ì–¸íŠ¸
llm_client = LLMClient()


async def call_llm(
    messages: List[Dict[str, str]],
    max_tokens: int = 1000,
    temperature: float = None,  # Noneì´ë©´ í™˜ê²½ ë³€ìˆ˜ ê¸°ë³¸ê°’ ì‚¬ìš©
    json_mode: bool = False
) -> str:
    """LLM í˜¸ì¶œ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return await llm_client.call(messages, max_tokens, temperature, json_mode)


# =============================================================================
# Step Status Enum
# =============================================================================

class StepStatus(str, Enum):
    """ì›Œí¬í”Œë¡œìš° ìŠ¤í… ìƒíƒœ"""
    PENDING = "pending"           # ëŒ€ê¸° ì¤‘
    RUNNING = "running"           # ì‹¤í–‰ ì¤‘
    WAITING_USER = "waiting_user" # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
    COMPLETED = "completed"       # ì™„ë£Œ
    FAILED = "failed"             # ì‹¤íŒ¨


# =============================================================================
# Agent Context & Result (Agent ê°„ ë°ì´í„° ì „ë‹¬)
# =============================================================================

@dataclass
class AgentContext:
    """Agent ì‹¤í–‰ ì»¨í…ìŠ¤íŠ¸"""
    task_id: str
    task_content: str
    step_description: str
    previous_results: List[Dict[str, Any]] = field(default_factory=list)
    user_inputs: Dict[str, str] = field(default_factory=dict)  # step_id -> input
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class AgentResult:
    """Agent ì‹¤í–‰ ê²°ê³¼ (Structured)"""
    success: bool
    output: str
    data: Optional[Dict[str, Any]] = None  # Structured data
    needs_user_input: bool = False
    user_prompt: str = ""
    error: Optional[str] = None


# =============================================================================
# Base Agent (Abstract)
# =============================================================================

class BaseAgent(ABC):
    """
    Agent ì¶”ìƒ í´ë˜ìŠ¤
    - ê° AgentëŠ” ê³ ìœ í•œ ì‹¤í–‰ ì „ëµ, Tool, Promptë¥¼ ê°€ì§
    """
    
    def __init__(self, agent_id: str, name: str, description: str = ""):
        self.id = agent_id
        self.name = name
        self.description = description
        self.system_prompt: str = ""
        self.tools: List[str] = []
    
    @abstractmethod
    async def run(self, context: AgentContext) -> AgentResult:
        """
        Agent ì‹¤í–‰ - ë°˜ë“œì‹œ êµ¬í˜„í•´ì•¼ í•¨
        """
        pass
    
    def get_system_prompt(self) -> str:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return self.system_prompt or f"ë‹¹ì‹ ì€ '{self.name}' Agentì…ë‹ˆë‹¤. {self.description}"


class LLMAgent(BaseAgent):
    """
    LLM ê¸°ë°˜ Agent - ê¸°ë³¸ êµ¬í˜„
    """
    
    def __init__(
        self,
        agent_id: str,
        name: str,
        description: str = "",
        system_prompt: str = "",
        output_schema: Optional[Dict[str, Any]] = None
    ):
        super().__init__(agent_id, name, description)
        self.system_prompt = system_prompt
        self.output_schema = output_schema  # JSON Schema for structured output
    
    async def run(self, context: AgentContext) -> AgentResult:
        """LLMì„ í†µí•œ Agent ì‹¤í–‰"""
        try:
            # ì´ì „ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í¬í•¨
            prev_results_text = ""
            if context.previous_results:
                prev_results_text = "\n\n**ì´ì „ ì‘ì—… ê²°ê³¼:**\n" + "\n".join([
                    f"- {r.get('agent', 'Agent')}: {r.get('result', '')}"
                    for r in context.previous_results
                ])
            
            # ì‚¬ìš©ì ì…ë ¥ í¬í•¨
            user_inputs_text = ""
            if context.user_inputs:
                user_inputs_text = "\n\n**ì‚¬ìš©ì ì…ë ¥:**\n" + "\n".join([
                    f"- {k}: {v}" for k, v in context.user_inputs.items()
                ])
            
            messages = [
                {
                    "role": "system",
                    "content": self.get_system_prompt()
                },
                {
                    "role": "user",
                    "content": f"""ë‹¤ìŒ ì‘ì—…ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”:

**ìš”ì²­**: {context.task_content}
**ë‹´ë‹¹ ì‘ì—…**: {context.step_description}
{prev_results_text}
{user_inputs_text}

ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{"output": "ì‘ì—… ê²°ê³¼", "data": {{"key": "value"}}}}"""
                }
            ]
            
            print(f"[LLMAgent] {self.name}: Calling LLM...")
            response = await call_llm(messages, max_tokens=4000, json_mode=True)
            print(f"[LLMAgent] {self.name}: Response = {response[:200] if response else 'EMPTY'}...")
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result_data = json.loads(response)
                output = result_data.get("output", response)
                print(f"[LLMAgent] {self.name}: Parsed output = {output[:100] if output else 'EMPTY'}...")
                return AgentResult(
                    success=True,
                    output=output,
                    data=result_data.get("data")
                )
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ raw output ë°˜í™˜
                print(f"[LLMAgent] {self.name}: JSON parse failed, using raw response")
                return AgentResult(
                    success=True,
                    output=response
                )
                
        except Exception as e:
            return AgentResult(
                success=False,
                output="",
                error=str(e)
            )


# =============================================================================
# Workflow Step & State
# =============================================================================

@dataclass
class WorkflowStep:
    """ì›Œí¬í”Œë¡œìš°ì˜ ê° ë‹¨ê³„"""
    id: str  # ê³ ìœ  ID
    agent_id: str
    agent_name: str
    description: str
    order: int
    needs_user_input: bool = False
    input_prompt: str = ""
    status: StepStatus = StepStatus.PENDING
    result: Optional[AgentResult] = None
    user_input: Optional[str] = None  # ì´ ìŠ¤í…ì— ëŒ€í•œ ì‚¬ìš©ì ì…ë ¥
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None


@dataclass
class WorkflowState:
    """ì§„í–‰ ì¤‘ì¸ ì›Œí¬í”Œë¡œìš° ìƒíƒœ"""
    task_id: str
    task_content: str
    steps: List[WorkflowStep]
    current_step_index: int = 0
    status: str = "running"  # running, waiting_user, completed, failed
    created_at: datetime = field(default_factory=datetime.now)
    
    def get_current_step(self) -> Optional[WorkflowStep]:
        if 0 <= self.current_step_index < len(self.steps):
            return self.steps[self.current_step_index]
        return None
    
    def is_completed(self) -> bool:
        return self.current_step_index >= len(self.steps)
    
    def advance(self) -> None:
        """ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì´ë™"""
        if not self.is_completed():
            self.current_step_index += 1
    
    def get_results(self) -> List[Dict[str, Any]]:
        """ì™„ë£Œëœ ìŠ¤í…ë“¤ì˜ ê²°ê³¼ ë°˜í™˜"""
        return [
            {
                "agent": step.agent_name,
                "result": step.result.output if step.result else "",
                "data": step.result.data if step.result else None
            }
            for step in self.steps
            if step.status == StepStatus.COMPLETED and step.result
        ]
    
    def get_user_inputs(self) -> Dict[str, str]:
        """ìŠ¤í…ë³„ ì‚¬ìš©ì ì…ë ¥ ë°˜í™˜"""
        return {
            step.id: step.user_input
            for step in self.steps
            if step.user_input is not None
        }


# =============================================================================
# Workflow Manager (Thread-safe)
# =============================================================================

class WorkflowManager:
    """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ê´€ë¦¬ì - ë™ì‹œì„± ì•ˆì „"""
    
    def __init__(self):
        self._workflows: Dict[str, WorkflowState] = {}
        self._locks: Dict[str, asyncio.Lock] = {}
        self._global_lock = asyncio.Lock()
    
    async def _get_lock(self, task_id: str) -> asyncio.Lock:
        """task_idë³„ Lock íšë“"""
        async with self._global_lock:
            if task_id not in self._locks:
                self._locks[task_id] = asyncio.Lock()
            return self._locks[task_id]
    
    async def create_workflow(
        self,
        task_id: str,
        task_content: str,
        steps: List[WorkflowStep]
    ) -> WorkflowState:
        """ìƒˆ ì›Œí¬í”Œë¡œìš° ìƒì„±"""
        lock = await self._get_lock(task_id)
        async with lock:
            workflow = WorkflowState(
                task_id=task_id,
                task_content=task_content,
                steps=steps
            )
            self._workflows[task_id] = workflow
            return workflow
    
    async def get_workflow(self, task_id: str) -> Optional[WorkflowState]:
        """ì›Œí¬í”Œë¡œìš° ì¡°íšŒ"""
        lock = await self._get_lock(task_id)
        async with lock:
            return self._workflows.get(task_id)
    
    async def has_pending_workflow(self, task_id: str) -> bool:
        """ëŒ€ê¸° ì¤‘ì¸ ì›Œí¬í”Œë¡œìš°ê°€ ìˆëŠ”ì§€ í™•ì¸"""
        lock = await self._get_lock(task_id)
        async with lock:
            workflow = self._workflows.get(task_id)
            return workflow is not None and workflow.status == "waiting_user"
    
    async def remove_workflow(self, task_id: str) -> Optional[WorkflowState]:
        """ì›Œí¬í”Œë¡œìš° ì œê±°"""
        lock = await self._get_lock(task_id)
        async with lock:
            workflow = self._workflows.pop(task_id, None)
            # Lockë„ ì •ë¦¬
            async with self._global_lock:
                self._locks.pop(task_id, None)
            return workflow
    
    async def update_step_status(
        self,
        task_id: str,
        step_index: int,
        status: StepStatus,
        result: Optional[AgentResult] = None
    ) -> None:
        """ìŠ¤í… ìƒíƒœ ì—…ë°ì´íŠ¸"""
        lock = await self._get_lock(task_id)
        async with lock:
            workflow = self._workflows.get(task_id)
            if workflow and 0 <= step_index < len(workflow.steps):
                step = workflow.steps[step_index]
                step.status = status
                if result:
                    step.result = result
                if status == StepStatus.RUNNING:
                    step.started_at = datetime.now()
                elif status in (StepStatus.COMPLETED, StepStatus.FAILED):
                    step.completed_at = datetime.now()
    
    async def add_user_input(self, task_id: str, user_input: str) -> None:
        """í˜„ì¬ ìŠ¤í…ì— ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€"""
        lock = await self._get_lock(task_id)
        async with lock:
            workflow = self._workflows.get(task_id)
            if workflow:
                current_step = workflow.get_current_step()
                if current_step:
                    current_step.user_input = user_input
                    current_step.status = StepStatus.COMPLETED
                workflow.status = "running"
    
    async def set_workflow_status(self, task_id: str, status: str) -> None:
        """ì›Œí¬í”Œë¡œìš° ìƒíƒœ ì„¤ì •"""
        lock = await self._get_lock(task_id)
        async with lock:
            workflow = self._workflows.get(task_id)
            if workflow:
                workflow.status = status


# =============================================================================
# Agent Registry
# =============================================================================

class AgentRegistry:
    """
    Agent ë ˆì§€ìŠ¤íŠ¸ë¦¬ - Agent ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
    """
    
    def __init__(self):
        self._agents: Dict[str, BaseAgent] = {}
    
    def register(self, agent: BaseAgent) -> None:
        """Agent ë“±ë¡"""
        self._agents[agent.id] = agent
    
    def get(self, agent_id: str) -> Optional[BaseAgent]:
        """Agent ì¡°íšŒ"""
        return self._agents.get(agent_id)
    
    def get_or_create_llm_agent(
        self,
        agent_id: str,
        name: str,
        description: str = ""
    ) -> BaseAgent:
        """Agent ì¡°íšŒ ë˜ëŠ” LLMAgent ìƒì„±"""
        if agent_id not in self._agents:
            self._agents[agent_id] = LLMAgent(
                agent_id=agent_id,
                name=name,
                description=description
            )
        return self._agents[agent_id]


# =============================================================================
# Orchestration Engine (Central Execution Loop)
# =============================================================================

class OrchestrationEngine:
    """
    ë©€í‹°-ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ì—”ì§„
    ì¤‘ì•™ ì‹¤í–‰ ë£¨í”„ + Agent ì‹¤í–‰ + ìƒíƒœ ê´€ë¦¬
    """
    
    def __init__(self, workflow_manager: WorkflowManager):
        self.workflow_manager = workflow_manager
        self.agent_registry = AgentRegistry()
        self.ws_server: Any = None
    
    def set_ws_server(self, ws_server: Any) -> None:
        """WebSocket ì„œë²„ ì„¤ì •"""
        self.ws_server = ws_server
    
    async def run_workflow(self, task_id: str) -> Optional[str]:
        """
        ì¤‘ì•™ ì‹¤í–‰ ë£¨í”„ - ì›Œí¬í”Œë¡œìš° ì „ì²´ ì‹¤í–‰
        Returns: ìµœì¢… ì‘ë‹µ ë˜ëŠ” None (ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°)
        """
        workflow = await self.workflow_manager.get_workflow(task_id)
        if not workflow:
            return None
        
        while not workflow.is_completed():
            step = workflow.get_current_step()
            if not step:
                break
            
            # ìŠ¤í… ì‹¤í–‰
            await self.workflow_manager.update_step_status(
                task_id, workflow.current_step_index, StepStatus.RUNNING
            )
            
            self._log(
                agent_id=step.agent_id,
                agent_name=step.agent_name,
                log_type="info",
                message=f"ğŸ”§ ì‘ì—… ì‹œì‘: {step.description}",
                details=f"Step {step.order}/{len(workflow.steps)}",
                task_id=task_id
            )
            
            # Agent ì‹¤í–‰
            agent = self.agent_registry.get_or_create_llm_agent(
                step.agent_id, step.agent_name, step.description
            )
            
            context = AgentContext(
                task_id=task_id,
                task_content=workflow.task_content,
                step_description=step.description,
                previous_results=workflow.get_results(),
                user_inputs=workflow.get_user_inputs()
            )
            
            result = await agent.run(context)
            
            if result.success:
                await self.workflow_manager.update_step_status(
                    task_id, workflow.current_step_index, StepStatus.COMPLETED, result
                )
                
                self._log(
                    agent_id=step.agent_id,
                    agent_name=step.agent_name,
                    log_type="info",
                    message=f"âœ… ì‘ì—… ì™„ë£Œ",
                    details=result.output[:100] + "..." if len(result.output) > 100 else result.output,
                    task_id=task_id
                )
            else:
                await self.workflow_manager.update_step_status(
                    task_id, workflow.current_step_index, StepStatus.FAILED, result
                )
                
                self._log(
                    agent_id=step.agent_id,
                    agent_name=step.agent_name,
                    log_type="error",
                    message=f"âŒ ì‘ì—… ì‹¤íŒ¨",
                    details=result.error or "Unknown error",
                    task_id=task_id
                )
            
            # ì‚¬ìš©ì ì…ë ¥ì´ í•„ìš”í•œ ê²½ìš°: ì¼ì‹œ ì¤‘ì§€
            if step.needs_user_input and step.input_prompt:
                await self.workflow_manager.update_step_status(
                    task_id, workflow.current_step_index, StepStatus.WAITING_USER
                )
                await self.workflow_manager.set_workflow_status(task_id, "waiting_user")
                
                self._log(
                    agent_id="question-agent-system",
                    agent_name="Question Agent",
                    log_type="info",
                    message="â“ ì‚¬ìš©ì ì…ë ¥ ìš”ì²­",
                    details=step.input_prompt,
                    task_id=task_id
                )
                
                # ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ í‘œì‹œ
                if self.ws_server:
                    self.ws_server.broadcast_task_interaction(
                        task_id=task_id,
                        role='agent',
                        message=step.input_prompt,
                        agent_id=step.agent_id,
                        agent_name=step.agent_name
                    )
                
                return None  # ì‚¬ìš©ì ì…ë ¥ ëŒ€ê¸°
            
            # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ
            workflow.advance()
        
        # ëª¨ë“  ìŠ¤í… ì™„ë£Œ: ìµœì¢… ì‘ë‹µ ìƒì„±
        return await self.generate_final_response(workflow, task_id)
    
    async def resume_workflow(self, task_id: str, user_input: str) -> Optional[str]:
        """
        ì›Œí¬í”Œë¡œìš° ì¬ê°œ - ì‚¬ìš©ì ì…ë ¥ í›„
        """
        await self.workflow_manager.add_user_input(task_id, user_input)
        
        workflow = await self.workflow_manager.get_workflow(task_id)
        if workflow:
            current_step = workflow.get_current_step()
            if current_step:
                self._log(
                    agent_id=current_step.agent_id,
                    agent_name=current_step.agent_name,
                    log_type="info",
                    message=f"âœ… ì‚¬ìš©ì ì…ë ¥ ìˆ˜ì‹ : {user_input}",
                    details="ì›Œí¬í”Œë¡œìš° ì¬ê°œ",
                    task_id=task_id
                )
            
            # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì§„í–‰
            workflow.advance()
        
        return await self.run_workflow(task_id)
    
    async def generate_final_response(
        self,
        workflow: WorkflowState,
        task_id: str
    ) -> str:
        """Answer Agentë¥¼ í†µí•œ ìµœì¢… ì‘ë‹µ ìƒì„±"""
        self._log(
            agent_id="answer-agent-system",
            agent_name="Answer Agent",
            log_type="info",
            message="ğŸ“ ìµœì¢… ì‘ë‹µ ìƒì„± ì¤‘...",
            details=f"ì²˜ë¦¬ëœ ê²°ê³¼: {len(workflow.get_results())}ê°œ",
            task_id=task_id
        )
        
        results = workflow.get_results()
        results_text = "\n".join([
            f"- {r['agent']}: {r['result']}"
            for r in results
        ])
        
        user_inputs = workflow.get_user_inputs()
        user_inputs_text = ""
        if user_inputs:
            user_inputs_text = "\n\nì‚¬ìš©ì ì…ë ¥:\n" + "\n".join([
                f"- {v}" for v in user_inputs.values()
            ])
        
        messages = [
            {
                "role": "system",
                "content": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‘ì—… ê²°ê³¼ë¥¼ ì‚¬ìš©ìì—ê²Œ ì•Œê¸° ì‰½ê²Œ ìš”ì•½í•´ì„œ ì „ë‹¬í•´ì£¼ì„¸ìš”. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ê³ , ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."
            },
            {
                "role": "user",
                "content": f"""ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ìœ ìš©í•œ ì‘ë‹µì„ ì‘ì„±í•´ì£¼ì„¸ìš”:

**ì›ë˜ ìš”ì²­**: {workflow.task_content}

**ì²˜ë¦¬ ê²°ê³¼**:
{results_text}
{user_inputs_text}

ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""
            }
        ]
        
        final_response = await call_llm(messages, max_tokens=4000)
        
        if not final_response or "error" in final_response.lower():
            final_response = f"âœ… ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\nğŸ“‹ ì²˜ë¦¬ ë‚´ì—­:\n{results_text}"
        
        # ë§ˆì§€ë§‰ Agent ì´ë¦„ìœ¼ë¡œ ì‘ë‹µ
        last_step = workflow.steps[-1] if workflow.steps else None
        display_id = last_step.agent_id if last_step else "answer-agent-system"
        display_name = last_step.agent_name if last_step else "Answer Agent"
        
        if self.ws_server:
            self.ws_server.broadcast_task_interaction(
                task_id=task_id,
                role='agent',
                message=final_response,
                agent_id=display_id,
                agent_name=display_name
            )
        
        self._log(
            agent_id="answer-agent-system",
            agent_name="Answer Agent",
            log_type="info",
            message="âœ… ìµœì¢… ì‘ë‹µ ì™„ë£Œ",
            details="ì‚¬ìš©ìì—ê²Œ ì‘ë‹µ ì „ì†¡ë¨",
            task_id=task_id
        )
        
        # ì›Œí¬í”Œë¡œìš° ì™„ë£Œ
        await self.workflow_manager.set_workflow_status(task_id, "completed")
        
        return final_response
    
    def _log(
        self,
        agent_id: str,
        agent_name: str,
        log_type: str,
        message: str,
        details: str = "",
        task_id: str = None
    ) -> None:
        """Agent Activity ë¡œê·¸"""
        if self.ws_server:
            self.ws_server.broadcast_agent_log(
                agent_id=agent_id,
                agent_name=agent_name,
                log_type=log_type,
                message=message,
                details=details,
                task_id=task_id
            )


# =============================================================================
# Utility Functions
# =============================================================================

def build_workflow_steps(
    planned_agents: List[Dict[str, Any]],
    agent_map: Dict[str, Any]
) -> List[WorkflowStep]:
    """
    í”„ë¡ íŠ¸ì—”ë“œ ê³„íš â†’ WorkflowStep ë¦¬ìŠ¤íŠ¸ ë³€í™˜
    """
    from uuid import uuid4
    
    steps = []
    for i, planned in enumerate(planned_agents):
        agent_id = planned.get('agentId')
        if agent_id in agent_map:
            agent = agent_map[agent_id]
            step = WorkflowStep(
                id=str(uuid4()),  # ê³ ìœ  ID
                agent_id=agent_id,
                agent_name=agent.name,
                description=planned.get('reason', planned.get('agentName', '')),
                order=planned.get('order', i + 1),
                needs_user_input=planned.get('needsUserInput', False),
                input_prompt=planned.get('inputPrompt', '')
            )
            steps.append(step)
    
    steps.sort(key=lambda s: s.order)
    return steps


# =============================================================================
# Global Instances
# =============================================================================

workflow_manager = WorkflowManager()
orchestration_engine = OrchestrationEngine(workflow_manager)
